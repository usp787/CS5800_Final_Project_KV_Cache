{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhfvLcef5mHeKg/uK+4hUu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usp787/CS5800_Final_Project_KV_Cache/blob/Code/kv_cache_code_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "StphgGPM3y-U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import psutil\n",
        "import os\n",
        "from typing import List, Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set style for better-looking plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ],
      "metadata": {
        "id": "Vm-H0DmnEYr0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATION - Easy to Edit\n",
        "\n",
        "CONFIG = {\n",
        "    # Experiment parameters\n",
        "    'output_lengths': [10, 50, 100, 250, 500],  # Token lengths to test\n",
        "    'trials_per_length': 3,                      # Number of trials per length\n",
        "    'initial_prompt': \"The future of artificial intelligence\",  # Starting prompt\n",
        "\n",
        "    # Generation parameters\n",
        "    'temperature': 0.7,\n",
        "    'top_k': 50,\n",
        "    'do_sample': True,\n",
        "\n",
        "    # Model\n",
        "    'model_name': 'distilgpt2'\n",
        "}"
      ],
      "metadata": {
        "id": "vQ2Zr06BEbrd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 3. Load Model and Tokenizer\n",
        "\n",
        "print(\"Loading model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "model = AutoModelForCausalLM.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "# Set padding token if not already set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model loaded on: {device}\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwKVkjZpEr3l",
        "outputId": "92d902e3-97f7-4372-e77a-4ed46548576e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer...\n",
            "Model loaded on: cuda\n",
            "Model parameters: 81,912,576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 4. Memory Tracking Utilities\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "def get_memory_usage() -> Dict[str, float]:\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "\n",
        "    memory_data = {\n",
        "        'ram_mb': memory_info.rss / 1024 / 1024,  # Resident Set Size\n",
        "    }\n",
        "\n",
        "    # GPU memory if available\n",
        "    if torch.cuda.is_available():\n",
        "        memory_data['gpu_allocated_mb'] = torch.cuda.memory_allocated() / 1024 / 1024\n",
        "        memory_data['gpu_reserved_mb'] = torch.cuda.memory_reserved() / 1024 / 1024\n",
        "\n",
        "    return memory_data\n",
        "\n",
        "def get_model_size() -> float:\n",
        "    \"\"\"Calculate model size in MB\"\"\"\n",
        "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
        "    return (param_size + buffer_size) / 1024 / 1024"
      ],
      "metadata": {
        "id": "LRE7Vn86E4co"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}