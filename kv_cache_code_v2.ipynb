{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPp14q4Jsf3apJCxrCFctrW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usp787/CS5800_Final_Project_KV_Cache/blob/Code/kv_cache_code_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StphgGPM3y-U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import psutil\n",
        "import os\n",
        "from typing import Dict, List\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    'output_lengths': [10, 25, 50, 75, 100, 250, 500, 750, 1000],\n",
        "    'trials_per_length': 5,  # Increased for better statistics\n",
        "    'warmup_runs': 3,  # Warmup runs per configuration\n",
        "    'initial_prompt': \"The future of artificial intelligence\",\n",
        "    'temperature': 0.7,\n",
        "    'top_k': 50,\n",
        "    'do_sample': True,\n",
        "    'model_name': 'distilgpt2'\n",
        "}"
      ],
      "metadata": {
        "id": "Vm-H0DmnEYr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model_and_tokenizer():\n",
        "    \"\"\"Load model and tokenizer\"\"\"\n",
        "    print(\"Loading model and tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "    model = AutoModelForCausalLM.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded on: {device}\")\n",
        "    print(f\"Model parameters: {model.num_parameters():,}\")\n",
        "\n",
        "    return model, tokenizer, device"
      ],
      "metadata": {
        "id": "vQ2Zr06BEbrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_memory_usage() -> Dict[str, float]:\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    memory_data = {}\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()  # Wait for all operations to complete\n",
        "        memory_data['gpu_allocated_mb'] = torch.cuda.memory_allocated() / 1024 / 1024\n",
        "        memory_data['gpu_reserved_mb'] = torch.cuda.memory_reserved() / 1024 / 1024\n",
        "\n",
        "    return memory_data"
      ],
      "metadata": {
        "id": "KSP5vduAzwVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "LRE7Vn86E4co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tokens(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    device,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int,\n",
        "    use_cache: bool,\n",
        "    measure_memory: bool = False\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generate tokens with or without KV cache\n",
        "\n",
        "    Args:\n",
        "        use_cache: If True, use KV cache; if False, disable it\n",
        "    \"\"\"\n",
        "    # Encode prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    prompt_length = input_ids.shape[1]\n",
        "\n",
        "    # Clear memory before measurement\n",
        "    if measure_memory:\n",
        "        clear_memory()\n",
        "        initial_memory = get_memory_usage()\n",
        "\n",
        "    # Set cache configuration\n",
        "    model.config.use_cache = use_cache\n",
        "\n",
        "    # Synchronize GPU before timing\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            use_cache=use_cache,\n",
        "            do_sample=CONFIG['do_sample'],\n",
        "            temperature=CONFIG['temperature'],\n",
        "            top_k=CONFIG['top_k'],\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            attention_mask=torch.ones_like(input_ids)  # Explicit attention mask\n",
        "        )\n",
        "\n",
        "    # Synchronize GPU after generation\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    generation_time = end_time - start_time\n",
        "\n",
        "    # Measure memory after generation\n",
        "    if measure_memory:\n",
        "        final_memory = get_memory_usage()\n",
        "        memory_used = {\n",
        "            key: final_memory.get(key, 0) - initial_memory.get(key, 0)\n",
        "            for key in initial_memory.keys()\n",
        "        }\n",
        "    else:\n",
        "        memory_used = {}\n",
        "\n",
        "    # Calculate metrics\n",
        "    total_tokens = output.shape[1]\n",
        "    tokens_generated = total_tokens - prompt_length\n",
        "\n",
        "    return {\n",
        "        'generation_time': generation_time,\n",
        "        'tokens_generated': tokens_generated,\n",
        "        'time_per_token': generation_time / tokens_generated if tokens_generated > 0 else 0,\n",
        "        'prompt_length': prompt_length,\n",
        "        'total_length': total_tokens,\n",
        "        'memory_used': memory_used,\n",
        "        'use_cache': use_cache\n",
        "    }"
      ],
      "metadata": {
        "id": "DBCMhKQwzzRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def warmup_model(model, tokenizer, device, max_new_tokens: int, use_cache: bool):\n",
        "    \"\"\"Warmup the model to avoid first-run overhead\"\"\"\n",
        "    print(f\"  Warming up (use_cache={use_cache})...\", end=\" \")\n",
        "    for _ in range(CONFIG['warmup_runs']):\n",
        "        _ = generate_tokens(\n",
        "            model, tokenizer, device,\n",
        "            CONFIG['initial_prompt'],\n",
        "            max_new_tokens,\n",
        "            use_cache,\n",
        "            measure_memory=False\n",
        "        )\n",
        "    clear_memory()\n",
        "    print(\"✓\")"
      ],
      "metadata": {
        "id": "MZ5KVNHL0Cv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments(model, tokenizer, device):\n",
        "    \"\"\"Run complete experimental suite\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STARTING KV CACHE COMPARISON EXPERIMENTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Prompt: '{CONFIG['initial_prompt']}'\")\n",
        "    print(f\"Output lengths: {CONFIG['output_lengths']}\")\n",
        "    print(f\"Trials per configuration: {CONFIG['trials_per_length']}\")\n",
        "    print(f\"Warmup runs: {CONFIG['warmup_runs']}\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for output_length in CONFIG['output_lengths']:\n",
        "        print(f\"\\n{'─'*80}\")\n",
        "        print(f\"Testing output length: {output_length} tokens\")\n",
        "        print(f\"{'─'*80}\")\n",
        "\n",
        "        # Test both with and without cache\n",
        "        for use_cache in [True, False]:\n",
        "            cache_str = \"WITH\" if use_cache else \"WITHOUT\"\n",
        "            print(f\"\\n{cache_str} KV Cache:\")\n",
        "\n",
        "            # Warmup for this configuration\n",
        "            warmup_model(model, tokenizer, device, output_length, use_cache)\n",
        "\n",
        "            # Run trials\n",
        "            for trial in range(CONFIG['trials_per_length']):\n",
        "                print(f\"  Trial {trial + 1}/{CONFIG['trials_per_length']}...\", end=\" \")\n",
        "\n",
        "                result = generate_tokens(\n",
        "                    model, tokenizer, device,\n",
        "                    CONFIG['initial_prompt'],\n",
        "                    output_length,\n",
        "                    use_cache,\n",
        "                    measure_memory=(trial == 0)  # Only measure memory on first trial\n",
        "                )\n",
        "\n",
        "                # Store results\n",
        "                result_record = {\n",
        "                    'output_length': output_length,\n",
        "                    'use_cache': use_cache,\n",
        "                    'trial': trial + 1,\n",
        "                    'generation_time_ms': result['generation_time'] * 1000,\n",
        "                    'time_per_token_ms': result['time_per_token'] * 1000,\n",
        "                    'tokens_generated': result['tokens_generated'],\n",
        "                }\n",
        "\n",
        "                # Add memory info if measured\n",
        "                if result['memory_used']:\n",
        "                    result_record.update({\n",
        "                        f'{k}': v for k, v in result['memory_used'].items()\n",
        "                    })\n",
        "\n",
        "                all_results.append(result_record)\n",
        "\n",
        "                print(f\"✓ {result['generation_time']*1000:.2f}ms ({result['time_per_token']*1000:.2f}ms/token)\")\n",
        "\n",
        "                # Small delay between trials\n",
        "                time.sleep(0.1)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPERIMENTS COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return pd.DataFrame(all_results)"
      ],
      "metadata": {
        "id": "hSfFRTX50Gal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_results(df: pd.DataFrame):\n",
        "    \"\"\"Analyze and visualize experimental results\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STATISTICAL ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Group by output length and cache usage\n",
        "    summary = df.groupby(['output_length', 'use_cache']).agg({\n",
        "        'generation_time_ms': ['mean', 'std', 'min', 'max'],\n",
        "        'time_per_token_ms': ['mean', 'std']\n",
        "    }).round(2)\n",
        "\n",
        "    print(\"\\nGeneration Time Summary (ms):\")\n",
        "    print(summary)\n",
        "\n",
        "    # Calculate speedup\n",
        "    print(\"\\n\" + \"─\"*80)\n",
        "    print(\"SPEEDUP ANALYSIS\")\n",
        "    print(\"─\"*80)\n",
        "\n",
        "    for length in df['output_length'].unique():\n",
        "        with_cache = df[(df['output_length'] == length) & (df['use_cache'] == True)]['time_per_token_ms'].mean()\n",
        "        without_cache = df[(df['output_length'] == length) & (df['use_cache'] == False)]['time_per_token_ms'].mean()\n",
        "        speedup = without_cache / with_cache\n",
        "\n",
        "        print(f\"\\nLength {length} tokens:\")\n",
        "        print(f\"  WITH cache:    {with_cache:.2f} ms/token\")\n",
        "        print(f\"  WITHOUT cache: {without_cache:.2f} ms/token\")\n",
        "        print(f\"  Speedup:       {speedup:.2f}x\")\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "1h_Y8s0u0JlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    model, tokenizer, device = setup_model_and_tokenizer()\n",
        "\n",
        "    # Run experiments\n",
        "    results_df = run_experiments(model, tokenizer, device)\n",
        "\n",
        "    # Save raw results\n",
        "    #results_df.to_csv('kv_cache_results.csv', index=False)\n",
        "    #print(\"\\nResults saved to 'kv_cache_results.csv'\")\n",
        "\n",
        "    # Analyze\n",
        "    summary = analyze_results(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "971d196aad4c4723986f9bd6f49c47f4",
            "a7993e046c2747adaa11ce585f4bd473",
            "4dc18fb5122d48ffb226936b460cbe59",
            "f93706ec93534aeaa8d47b198211717b",
            "cf101cc87954494a8c58cf42870d99ba",
            "0453390b3d5c440fa81162d0a981cdb6",
            "da4aad3d5e8c45fa8cab784b14bf22e4",
            "a168efee89e049fd81acd2fa9eae4e11",
            "ddca4cc51fe443b9b474eede4d520660",
            "cc57a5d3ca6f4b15bb8a057550574204",
            "218b2f941ae94886a1eb36b42de039a6",
            "3aa9c58d2a8b44c89cd1b220c7a1c115",
            "a1acf178947d42c3a42b1d4654d331ef",
            "9e4ac371ebca4d56abf7da88aec96e2e",
            "8feec5e01b6948eca5844b55365ee437",
            "be7e084d974545cdb9e8d4937b38ae01",
            "17047b686b9942878101dd27e86dccb7",
            "7fba4a4085af4c83aece37c2f1b64b4b",
            "932f00e8e5294c16810ff65f8d7a4544",
            "04d580262bf34d678ec76185261840a2",
            "e430b5b7a8644a6ba4f0799d88fe1948",
            "3109f041c335473d87da15a87ef6213c",
            "f3fdd40db29f4032a63bef9a7e1883e0",
            "23fc5dbd439f4a15a714dd2156bd64a3",
            "74ec9972173f4313ba89bdcf60a0bcfd",
            "34a26b5b4d524efdbf47d3617a089d11",
            "8942fb167cad49998e3ff91ea3696c2c",
            "a328fae60a6e40df8b8e262c8cb3d3db",
            "d1037f3481784ddfa912b71b54798847",
            "c3cb4a1a28df44638e4104a090222166",
            "be0ea86493e34e96b6242726117624aa",
            "0ae9155293aa4ae684cfe5c95a1ff501",
            "d3a071c0a3fc4064863610e85cc0a55b",
            "786a20c0a82e4fb8bdc47450063aa36c",
            "79c4dd4cb9034b39816f445c94400500",
            "627b66750e544be8816e319c0e0d1a60",
            "df48bb291757409dba173b8adf45eaf0",
            "3d4cafa2f8a7424ab6962320d3e02347",
            "7ec1bdd1b01a41469fb2c5c63cee2ee0",
            "f38f08ac3be14c99a836344378b17b09",
            "2cdb65e17384424096bf51ad45fd2088",
            "fa3f957b5a4d4ed5852d5bb284846dae",
            "b59e3d62ca1f4e849c3b79c8448eb96f",
            "bc68d3ccb4684114a11601b7a49c7105",
            "24ae062b5d014c65978c07b1cf382237",
            "8475e081c9c14f81985937b1b8bf0c7d",
            "c59c684f10ad4ad3a19578d74dedf6ef",
            "5edf9b5008cd4a23b0fb51d31fe25100",
            "99a8951df8cb4d5e8b7e246955a8e7c1",
            "d59dcc8e2f4c4d8da33ef3e038ef0956",
            "45745471dd4547c4b284c5eacdead8e7",
            "774b562bad014a43982e096c8dda579c",
            "fbfca637fa0549ce93841d79bdaed520",
            "6abe48008459455b93e39a9ce55167ef",
            "6362606986f64385be90b4a6823029d8",
            "ced76134fd9a49ac926a2f9ad025cbbc",
            "0fec48c14d3246f39ea16d816223dfc6",
            "51f8b10ac07142d18dd3111b1110c649",
            "8c37b3fe00b44de58656b00781c029e8",
            "392c589742b14d3fb7e1e6ae592827ef",
            "8a54e7f809da4d8f8555d42554693b02",
            "e65f2d7a74de48da9126d82b2bcd8f44",
            "f5fe5df60ac64dbfb8f71bdc6f52caae",
            "70338ff4b0be4074965dd6698ad6b058",
            "ded9010076844cfdb6a7937624c45229",
            "b3862455dd44428184c8acc6c97412eb",
            "c11a30e046ee482a99101dc331de4be1",
            "7182e49969d34fcda3698adecc0c41f2",
            "7b1c430443f643bb87fd219c5a2698cf",
            "268b083b287141289c165c1549b2abae",
            "5fdb118f337245829d9f4a637db49f2c",
            "a41cdae1d3a24aa2aa90e8f6fa39cb3d",
            "09daae042bb04765992d54ce27a58cea",
            "3dea0d1423e64ceeb6e3877cea33d19e",
            "cb61d08fbf5b42e2b499142cf0c77f51",
            "3875cc4f13d749fdaf5298c53e0fada0",
            "72b5e4b776f340e5b60cb4c4cb50b045"
          ]
        },
        "id": "h_dCVQLP0NMI",
        "outputId": "250ae4bd-f601-42f5-f83f-dcaaba621bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "971d196aad4c4723986f9bd6f49c47f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3aa9c58d2a8b44c89cd1b220c7a1c115"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3fdd40db29f4032a63bef9a7e1883e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "786a20c0a82e4fb8bdc47450063aa36c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24ae062b5d014c65978c07b1cf382237"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ced76134fd9a49ac926a2f9ad025cbbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c11a30e046ee482a99101dc331de4be1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on: cuda\n",
            "Model parameters: 81,912,576\n",
            "\n",
            "================================================================================\n",
            "STARTING KV CACHE COMPARISON EXPERIMENTS\n",
            "================================================================================\n",
            "Prompt: 'The future of artificial intelligence'\n",
            "Output lengths: [10, 25, 50, 75, 100, 250, 500, 750, 1000]\n",
            "Trials per configuration: 5\n",
            "Warmup runs: 3\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Testing output length: 10 tokens\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "WITH KV Cache:\n",
            "  Warming up (use_cache=True)... ✓\n",
            "  Trial 1/5... ✓ 80.02ms (8.00ms/token)\n",
            "  Trial 2/5... ✓ 56.01ms (5.60ms/token)\n",
            "  Trial 3/5... ✓ 55.91ms (5.59ms/token)\n",
            "  Trial 4/5... ✓ 58.61ms (5.86ms/token)\n",
            "  Trial 5/5... ✓ 68.72ms (6.87ms/token)\n",
            "\n",
            "WITHOUT KV Cache:\n",
            "  Warming up (use_cache=False)... ✓\n",
            "  Trial 1/5... ✓ 59.18ms (5.92ms/token)\n",
            "  Trial 2/5... ✓ 57.00ms (5.70ms/token)\n",
            "  Trial 3/5... ✓ 56.90ms (5.69ms/token)\n",
            "  Trial 4/5... ✓ 60.08ms (6.01ms/token)\n",
            "  Trial 5/5... ✓ 57.25ms (5.73ms/token)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Testing output length: 25 tokens\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "WITH KV Cache:\n",
            "  Warming up (use_cache=True)... ✓\n",
            "  Trial 1/5... ✓ 139.44ms (5.58ms/token)\n",
            "  Trial 2/5... ✓ 140.57ms (5.62ms/token)\n",
            "  Trial 3/5... ✓ 139.15ms (5.57ms/token)\n",
            "  Trial 4/5... ✓ 135.43ms (5.42ms/token)\n",
            "  Trial 5/5... ✓ 138.26ms (5.53ms/token)\n",
            "\n",
            "WITHOUT KV Cache:\n",
            "  Warming up (use_cache=False)... ✓\n",
            "  Trial 1/5... ✓ 15.48ms (7.74ms/token)\n",
            "  Trial 2/5... ✓ 161.21ms (6.45ms/token)\n",
            "  Trial 3/5... ✓ 154.57ms (6.18ms/token)\n",
            "  Trial 4/5... ✓ 147.19ms (5.89ms/token)\n",
            "  Trial 5/5... ✓ 146.45ms (5.86ms/token)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Testing output length: 50 tokens\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "WITH KV Cache:\n",
            "  Warming up (use_cache=True)... ✓\n",
            "  Trial 1/5... ✓ 267.28ms (5.35ms/token)\n",
            "  Trial 2/5... ✓ 266.37ms (5.33ms/token)\n",
            "  Trial 3/5... ✓ 381.58ms (7.63ms/token)\n",
            "  Trial 4/5... ✓ 342.01ms (6.84ms/token)\n",
            "  Trial 5/5... ✓ 155.69ms (6.77ms/token)\n",
            "\n",
            "WITHOUT KV Cache:\n",
            "  Warming up (use_cache=False)... ✓\n",
            "  Trial 1/5... ✓ 278.28ms (5.57ms/token)\n",
            "  Trial 2/5... ✓ 40.08ms (6.68ms/token)\n",
            "  Trial 3/5... ✓ 293.40ms (5.87ms/token)\n",
            "  Trial 4/5... ✓ 283.15ms (5.66ms/token)\n",
            "  Trial 5/5... ✓ 74.14ms (5.70ms/token)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Testing output length: 75 tokens\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "WITH KV Cache:\n",
            "  Warming up (use_cache=True)... ✓\n",
            "  Trial 1/5... ✓ 345.80ms (5.40ms/token)\n",
            "  Trial 2/5... ✓ 48.73ms (5.41ms/token)\n",
            "  Trial 3/5... ✓ 21.72ms (10.86ms/token)\n",
            "  Trial 4/5... ✓ 397.85ms (5.30ms/token)\n",
            "  Trial 5/5... ✓ 393.48ms (5.25ms/token)\n",
            "\n",
            "WITHOUT KV Cache:\n",
            "  Warming up (use_cache=False)... ✓\n",
            "  Trial 1/5... ✓ 380.81ms (5.95ms/token)\n",
            "  Trial 2/5... ✓ 428.91ms (5.72ms/token)\n",
            "  Trial 3/5... ✓ 376.37ms (5.88ms/token)\n",
            "  Trial 4/5... ✓ 436.49ms (5.82ms/token)\n",
            "  Trial 5/5... ✓ 155.90ms (6.50ms/token)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Testing output length: 100 tokens\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "WITH KV Cache:\n",
            "  Warming up (use_cache=True)... ✓\n",
            "  Trial 1/5... ✓ 665.72ms (6.66ms/token)\n",
            "  Trial 2/5... ✓ 561.80ms (5.62ms/token)\n",
            "  Trial 3/5... ✓ 2146.76ms (21.47ms/token)\n",
            "  Trial 4/5... ✓ 1941.76ms (19.42ms/token)\n",
            "  Trial 5/5... ✓ 1830.26ms (18.30ms/token)\n",
            "\n",
            "WITHOUT KV Cache:\n",
            "  Warming up (use_cache=False)... ✓\n",
            "  Trial 1/5... ✓ 208.24ms (9.05ms/token)\n",
            "  Trial 2/5... ✓ 689.14ms (6.89ms/token)\n",
            "  Trial 3/5... ✓ 575.17ms (5.75ms/token)\n",
            "  Trial 4/5... ✓ 578.75ms (5.79ms/token)\n",
            "  Trial 5/5... ✓ 565.18ms (5.65ms/token)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Testing output length: 250 tokens\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "WITH KV Cache:\n",
            "  Warming up (use_cache=True)... ✓\n",
            "  Trial 1/5... ✓ 1328.11ms (5.33ms/token)\n",
            "  Trial 2/5... ✓ 196.06ms (5.30ms/token)\n",
            "  Trial 3/5... ✓ 1342.67ms (5.37ms/token)\n",
            "  Trial 4/5... ✓ 1121.78ms (5.61ms/token)\n",
            "  Trial 5/5... ✓ 1720.83ms (6.88ms/token)\n",
            "\n",
            "WITHOUT KV Cache:\n",
            "  Warming up (use_cache=False)... ✓\n",
            "  Trial 1/5... ✓ 77.10ms (5.93ms/token)\n",
            "  Trial 2/5... ✓ 1692.70ms (6.77ms/token)\n",
            "  Trial 3/5... ✓ 1665.36ms (6.66ms/token)\n",
            "  Trial 4/5... ✓ 245.04ms (5.83ms/token)\n",
            "  Trial 5/5... ✓ 1700.36ms (6.80ms/token)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Testing output length: 500 tokens\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "WITH KV Cache:\n",
            "  Warming up (use_cache=True)... ✓\n",
            "  Trial 1/5... ✓ 163.13ms (5.44ms/token)\n",
            "  Trial 2/5... ✓ 2857.85ms (5.72ms/token)\n",
            "  Trial 3/5... ✓ 3010.93ms (6.02ms/token)\n",
            "  Trial 4/5... ✓ 76.08ms (5.43ms/token)\n",
            "  Trial 5/5... ✓ 2680.08ms (5.36ms/token)\n",
            "\n",
            "WITHOUT KV Cache:\n",
            "  Warming up (use_cache=False)... ✓\n",
            "  Trial 1/5... ✓ 5501.60ms (11.00ms/token)\n",
            "  Trial 2/5... ✓ 5258.52ms (10.52ms/token)\n",
            "  Trial 3/5... ✓ 5551.92ms (11.10ms/token)\n",
            "  Trial 4/5... ✓ 23.85ms (5.96ms/token)\n",
            "  Trial 5/5... ✓ 91.96ms (5.75ms/token)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Testing output length: 750 tokens\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "WITH KV Cache:\n",
            "  Warming up (use_cache=True)... ✓\n",
            "  Trial 1/5... ✓ 15.19ms (7.59ms/token)\n",
            "  Trial 2/5... ✓ 4000.83ms (5.33ms/token)\n",
            "  Trial 3/5... ✓ 4365.49ms (5.82ms/token)\n",
            "  Trial 4/5... ✓ 4143.32ms (5.52ms/token)\n",
            "  Trial 5/5... ✓ 3975.90ms (5.30ms/token)\n",
            "\n",
            "WITHOUT KV Cache:\n",
            "  Warming up (use_cache=False)... ✓\n",
            "  Trial 1/5... ✓ 581.57ms (5.87ms/token)\n",
            "  Trial 2/5... ✓ 104.02ms (5.78ms/token)\n",
            "  Trial 3/5... ✓ 122.11ms (5.81ms/token)\n",
            "  Trial 4/5... ✓ 10603.84ms (14.14ms/token)\n",
            "  Trial 5/5... ✓ 10753.72ms (14.34ms/token)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Testing output length: 1000 tokens\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "WITH KV Cache:\n",
            "  Warming up (use_cache=True)... ✓\n",
            "  Trial 1/5... ✓ 5846.79ms (5.85ms/token)\n",
            "  Trial 2/5... ✓ 5308.99ms (5.31ms/token)\n",
            "  Trial 3/5... ✓ 214.91ms (5.24ms/token)\n",
            "  Trial 4/5... ✓ 280.13ms (5.29ms/token)\n",
            "  Trial 5/5... ✓ 5890.88ms (5.89ms/token)\n",
            "\n",
            "WITHOUT KV Cache:\n",
            "  Warming up (use_cache=False)... ✓\n",
            "  Trial 1/5... ✓ 5940.06ms (11.36ms/token)\n",
            "  Trial 2/5... ✓ 1516.23ms (6.62ms/token)\n",
            "  Trial 3/5... ✓ 18520.79ms (18.52ms/token)\n",
            "  Trial 4/5... ✓ 126.15ms (6.01ms/token)\n",
            "  Trial 5/5... ✓ 18147.10ms (18.15ms/token)\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENTS COMPLETE\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STATISTICAL ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Generation Time Summary (ms):\n",
            "                        generation_time_ms                             \\\n",
            "                                      mean      std     min       max   \n",
            "output_length use_cache                                                 \n",
            "10            False                  58.08     1.45   56.90     60.08   \n",
            "              True                   63.85    10.46   55.91     80.02   \n",
            "25            False                 124.98    61.51   15.48    161.21   \n",
            "              True                  138.57     1.94  135.43    140.57   \n",
            "50            False                 193.81   125.49   40.08    293.40   \n",
            "              True                  282.59    86.50  155.69    381.58   \n",
            "75            False                 355.70   114.96  155.90    436.49   \n",
            "              True                  241.52   189.66   21.72    397.85   \n",
            "100           False                 523.30   183.22  208.24    689.14   \n",
            "              True                 1429.26   753.95  561.80   2146.76   \n",
            "250           False                1076.11   837.53   77.10   1700.36   \n",
            "              True                 1141.89   571.22  196.06   1720.83   \n",
            "500           False                3285.57  2948.63   23.85   5551.92   \n",
            "              True                 1757.61  1500.18   76.08   3010.93   \n",
            "750           False                4433.05  5705.00  104.02  10753.72   \n",
            "              True                 3300.14  1842.85   15.19   4365.49   \n",
            "1000          False                8850.07  8920.73  126.15  18520.79   \n",
            "              True                 3508.34  2985.60  214.91   5890.88   \n",
            "\n",
            "                        time_per_token_ms        \n",
            "                                     mean   std  \n",
            "output_length use_cache                          \n",
            "10            False                  5.81  0.15  \n",
            "              True                   6.39  1.05  \n",
            "25            False                  6.42  0.77  \n",
            "              True                   5.54  0.08  \n",
            "50            False                  5.90  0.45  \n",
            "              True                   6.38  1.01  \n",
            "75            False                  5.97  0.30  \n",
            "              True                   6.45  2.47  \n",
            "100           False                  6.63  1.45  \n",
            "              True                  14.29  7.54  \n",
            "250           False                  6.40  0.48  \n",
            "              True                   5.70  0.67  \n",
            "500           False                  8.87  2.76  \n",
            "              True                   5.59  0.27  \n",
            "750           False                  9.19  4.61  \n",
            "              True                   5.91  0.96  \n",
            "1000          False                 12.13  6.03  \n",
            "              True                   5.51  0.32  \n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "SPEEDUP ANALYSIS\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Length 10 tokens:\n",
            "  WITH cache:    6.39 ms/token\n",
            "  WITHOUT cache: 5.81 ms/token\n",
            "  Speedup:       0.91x\n",
            "\n",
            "Length 25 tokens:\n",
            "  WITH cache:    5.54 ms/token\n",
            "  WITHOUT cache: 6.42 ms/token\n",
            "  Speedup:       1.16x\n",
            "\n",
            "Length 50 tokens:\n",
            "  WITH cache:    6.38 ms/token\n",
            "  WITHOUT cache: 5.90 ms/token\n",
            "  Speedup:       0.92x\n",
            "\n",
            "Length 75 tokens:\n",
            "  WITH cache:    6.45 ms/token\n",
            "  WITHOUT cache: 5.97 ms/token\n",
            "  Speedup:       0.93x\n",
            "\n",
            "Length 100 tokens:\n",
            "  WITH cache:    14.29 ms/token\n",
            "  WITHOUT cache: 6.63 ms/token\n",
            "  Speedup:       0.46x\n",
            "\n",
            "Length 250 tokens:\n",
            "  WITH cache:    5.70 ms/token\n",
            "  WITHOUT cache: 6.40 ms/token\n",
            "  Speedup:       1.12x\n",
            "\n",
            "Length 500 tokens:\n",
            "  WITH cache:    5.59 ms/token\n",
            "  WITHOUT cache: 8.87 ms/token\n",
            "  Speedup:       1.59x\n",
            "\n",
            "Length 750 tokens:\n",
            "  WITH cache:    5.91 ms/token\n",
            "  WITHOUT cache: 9.19 ms/token\n",
            "  Speedup:       1.55x\n",
            "\n",
            "Length 1000 tokens:\n",
            "  WITH cache:    5.51 ms/token\n",
            "  WITHOUT cache: 12.13 ms/token\n",
            "  Speedup:       2.20x\n"
          ]
        }
      ]
    }
  ]
}
