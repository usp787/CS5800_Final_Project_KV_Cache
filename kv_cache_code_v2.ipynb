{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbvTDeoT2g3eO+FE6kGaHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usp787/CS5800_Final_Project_KV_Cache/blob/Code/kv_cache_code_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "StphgGPM3y-U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import psutil\n",
        "import os\n",
        "from typing import List, Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set style for better-looking plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ],
      "metadata": {
        "id": "Vm-H0DmnEYr0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATION - Easy to Edit\n",
        "\n",
        "CONFIG = {\n",
        "    # Experiment parameters\n",
        "    'output_lengths': [10, 50, 100, 250, 500],  # Token lengths to test\n",
        "    'trials_per_length': 3,                      # Number of trials per length\n",
        "    'initial_prompt': \"The future of artificial intelligence\",  # Starting prompt\n",
        "\n",
        "    # Generation parameters\n",
        "    'temperature': 0.7,\n",
        "    'top_k': 50,\n",
        "    'do_sample': True,\n",
        "\n",
        "    # Model\n",
        "    'model_name': 'distilgpt2'\n",
        "}"
      ],
      "metadata": {
        "id": "vQ2Zr06BEbrd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 3. Load Model and Tokenizer\n",
        "\n",
        "print(\"Loading model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "model = AutoModelForCausalLM.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "# Set padding token if not already set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model loaded on: {device}\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwKVkjZpEr3l",
        "outputId": "92d902e3-97f7-4372-e77a-4ed46548576e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer...\n",
            "Model loaded on: cuda\n",
            "Model parameters: 81,912,576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 4. Memory Tracking Utilities\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "def get_memory_usage() -> Dict[str, float]:\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "\n",
        "    memory_data = {\n",
        "        'ram_mb': memory_info.rss / 1024 / 1024,  # Resident Set Size\n",
        "    }\n",
        "\n",
        "    # GPU memory if available\n",
        "    if torch.cuda.is_available():\n",
        "        memory_data['gpu_allocated_mb'] = torch.cuda.memory_allocated() / 1024 / 1024\n",
        "        memory_data['gpu_reserved_mb'] = torch.cuda.memory_reserved() / 1024 / 1024\n",
        "\n",
        "    return memory_data\n",
        "\n",
        "def get_model_size() -> float:\n",
        "    \"\"\"Calculate model size in MB\"\"\"\n",
        "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
        "    return (param_size + buffer_size) / 1024 / 1024"
      ],
      "metadata": {
        "id": "LRE7Vn86E4co"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 5. Token Generation WITHOUT KV Cache\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "def generate_without_cache(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int,\n",
        "    measure_memory: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generate tokens WITHOUT KV cache and measure performance\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with timing and memory metrics\n",
        "    \"\"\"\n",
        "    # Encode prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    prompt_length = input_ids.shape[1]\n",
        "\n",
        "    # Get initial memory\n",
        "    if measure_memory:\n",
        "        initial_memory = get_memory_usage()\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "    # Generate WITHOUT cache\n",
        "    model.config.use_cache = False  # Disable KV cache\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            use_cache=False,  # CRITICAL: No KV cache\n",
        "            do_sample=CONFIG['do_sample'],\n",
        "            temperature=CONFIG['temperature'],\n",
        "            top_k=CONFIG['top_k'],\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    generation_time = end_time - start_time\n",
        "\n",
        "    # Get final memory\n",
        "    if measure_memory:\n",
        "        final_memory = get_memory_usage()\n",
        "        memory_used = {\n",
        "            key: final_memory.get(key, 0) - initial_memory.get(key, 0)\n",
        "            for key in initial_memory.keys()\n",
        "        }\n",
        "    else:\n",
        "        memory_used = {}\n",
        "\n",
        "    # Calculate tokens generated\n",
        "    total_tokens = output.shape[1]\n",
        "    tokens_generated = total_tokens - prompt_length\n",
        "\n",
        "    # Decode output (optional - for verification)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return {\n",
        "        'generation_time': generation_time,\n",
        "        'tokens_generated': tokens_generated,\n",
        "        'time_per_token': generation_time / tokens_generated if tokens_generated > 0 else 0,\n",
        "        'prompt_length': prompt_length,\n",
        "        'total_length': total_tokens,\n",
        "        'memory_used': memory_used,\n",
        "        'generated_text': generated_text\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Wa-PTn9EKUQS"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}