{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoNk+YRuZuOndFoxmLDBxT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usp787/CS5800_Final_Project_KV_Cache/blob/Code/kv_cache_code_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "StphgGPM3y-U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import psutil\n",
        "import os\n",
        "from typing import List, Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set style for better-looking plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ],
      "metadata": {
        "id": "Vm-H0DmnEYr0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATION - Easy to Edit\n",
        "\n",
        "CONFIG = {\n",
        "    # Experiment parameters\n",
        "    'output_lengths': [10, 50, 100, 250, 500],  # Token lengths to test\n",
        "    'trials_per_length': 3,                      # Number of trials per length\n",
        "    'initial_prompt': \"The future of artificial intelligence\",  # Starting prompt\n",
        "\n",
        "    # Generation parameters\n",
        "    'temperature': 0.7,\n",
        "    'top_k': 50,\n",
        "    'do_sample': True,\n",
        "\n",
        "    # Model\n",
        "    'model_name': 'distilgpt2'\n",
        "}"
      ],
      "metadata": {
        "id": "vQ2Zr06BEbrd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 3. Load Model and Tokenizer\n",
        "\n",
        "print(\"Loading model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "model = AutoModelForCausalLM.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "# Set padding token if not already set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model loaded on: {device}\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwKVkjZpEr3l",
        "outputId": "92d902e3-97f7-4372-e77a-4ed46548576e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer...\n",
            "Model loaded on: cuda\n",
            "Model parameters: 81,912,576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 4. Memory Tracking Utilities\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "def get_memory_usage() -> Dict[str, float]:\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "\n",
        "    memory_data = {\n",
        "        'ram_mb': memory_info.rss / 1024 / 1024,  # Resident Set Size\n",
        "    }\n",
        "\n",
        "    # GPU memory if available\n",
        "    if torch.cuda.is_available():\n",
        "        memory_data['gpu_allocated_mb'] = torch.cuda.memory_allocated() / 1024 / 1024\n",
        "        memory_data['gpu_reserved_mb'] = torch.cuda.memory_reserved() / 1024 / 1024\n",
        "\n",
        "    return memory_data\n",
        "\n",
        "def get_model_size() -> float:\n",
        "    \"\"\"Calculate model size in MB\"\"\"\n",
        "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
        "    return (param_size + buffer_size) / 1024 / 1024"
      ],
      "metadata": {
        "id": "LRE7Vn86E4co"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 5. Token Generation WITHOUT KV Cache\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "def generate_without_cache(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int,\n",
        "    measure_memory: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generate tokens WITHOUT KV cache and measure performance\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with timing and memory metrics\n",
        "    \"\"\"\n",
        "    # Encode prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    prompt_length = input_ids.shape[1]\n",
        "\n",
        "    # Get initial memory\n",
        "    if measure_memory:\n",
        "        initial_memory = get_memory_usage()\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "    # Generate WITHOUT cache\n",
        "    model.config.use_cache = False  # Disable KV cache\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            use_cache=False,  # CRITICAL: No KV cache\n",
        "            do_sample=CONFIG['do_sample'],\n",
        "            temperature=CONFIG['temperature'],\n",
        "            top_k=CONFIG['top_k'],\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    generation_time = end_time - start_time\n",
        "\n",
        "    # Get final memory\n",
        "    if measure_memory:\n",
        "        final_memory = get_memory_usage()\n",
        "        memory_used = {\n",
        "            key: final_memory.get(key, 0) - initial_memory.get(key, 0)\n",
        "            for key in initial_memory.keys()\n",
        "        }\n",
        "    else:\n",
        "        memory_used = {}\n",
        "\n",
        "    # Calculate tokens generated\n",
        "    total_tokens = output.shape[1]\n",
        "    tokens_generated = total_tokens - prompt_length\n",
        "\n",
        "    # Decode output (optional - for verification)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return {\n",
        "        'generation_time': generation_time,\n",
        "        'tokens_generated': tokens_generated,\n",
        "        'time_per_token': generation_time / tokens_generated if tokens_generated > 0 else 0,\n",
        "        'prompt_length': prompt_length,\n",
        "        'total_length': total_tokens,\n",
        "        'memory_used': memory_used,\n",
        "        'generated_text': generated_text\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Wa-PTn9EKUQS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 6. Run Experiments\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STARTING EXPERIMENTS - WITHOUT KV CACHE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nInitial Prompt: '{CONFIG['initial_prompt']}'\")\n",
        "print(f\"Output Lengths: {CONFIG['output_lengths']}\")\n",
        "print(f\"Trials per Length: {CONFIG['trials_per_length']}\")\n",
        "print(f\"Model Size: {get_model_size():.2f} MB\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Store all results\n",
        "all_results = []\n",
        "\n",
        "# Run experiments for each output length\n",
        "for output_length in CONFIG['output_lengths']:\n",
        "    print(f\"\\n--- Testing Output Length: {output_length} tokens ---\")\n",
        "\n",
        "    for trial in range(CONFIG['trials_per_length']):\n",
        "        print(f\"  Trial {trial + 1}/{CONFIG['trials_per_length']}...\", end=\" \")\n",
        "\n",
        "        # Run generation\n",
        "        result = generate_without_cache(\n",
        "            CONFIG['initial_prompt'],\n",
        "            output_length,\n",
        "            measure_memory=True\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        result_record = {\n",
        "            'output_length': output_length,\n",
        "            'trial': trial + 1,\n",
        "            'generation_time_ms': result['generation_time'] * 1000,\n",
        "            'time_per_token_ms': result['time_per_token'] * 1000,\n",
        "            'tokens_generated': result['tokens_generated'],\n",
        "            'prompt_length': result['prompt_length'],\n",
        "            'ram_used_mb': result['memory_used'].get('ram_mb', 0),\n",
        "        }\n",
        "\n",
        "        # Add GPU memory if available\n",
        "        if 'gpu_allocated_mb' in result['memory_used']:\n",
        "            result_record['gpu_allocated_mb'] = result['memory_used']['gpu_allocated_mb']\n",
        "            result_record['gpu_reserved_mb'] = result['memory_used']['gpu_reserved_mb']\n",
        "\n",
        "        all_results.append(result_record)\n",
        "\n",
        "        print(f\"✓ {result['generation_time']*1000:.2f}ms ({result['time_per_token']*1000:.2f}ms/token)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXPERIMENTS COMPLETE\")\n",
        "print(\"=\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ET1meMvPdvD",
        "outputId": "34234be9-47ff-4a49-dd7a-f8366de3cd66"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STARTING EXPERIMENTS - WITHOUT KV CACHE\n",
            "================================================================================\n",
            "\n",
            "Initial Prompt: 'The future of artificial intelligence'\n",
            "Output Lengths: [10, 50, 100, 250, 500]\n",
            "Trials per Length: 3\n",
            "Model Size: 318.47 MB\n",
            "\n",
            "\n",
            "\n",
            "--- Testing Output Length: 10 tokens ---\n",
            "  Trial 1/3... ✓ 135.58ms (13.56ms/token)\n",
            "  Trial 2/3... ✓ 155.91ms (15.59ms/token)\n",
            "  Trial 3/3... ✓ 175.58ms (17.56ms/token)\n",
            "\n",
            "--- Testing Output Length: 50 tokens ---\n",
            "  Trial 1/3... ✓ 861.40ms (17.23ms/token)\n",
            "  Trial 2/3... ✓ 355.00ms (7.10ms/token)\n",
            "  Trial 3/3... ✓ 282.32ms (5.65ms/token)\n",
            "\n",
            "--- Testing Output Length: 100 tokens ---\n",
            "  Trial 1/3... ✓ 132.74ms (5.53ms/token)\n",
            "  Trial 2/3... ✓ 581.77ms (5.82ms/token)\n",
            "  Trial 3/3... ✓ 565.56ms (5.66ms/token)\n",
            "\n",
            "--- Testing Output Length: 250 tokens ---\n",
            "  Trial 1/3... ✓ 1607.90ms (6.43ms/token)\n",
            "  Trial 2/3... ✓ 1633.66ms (6.53ms/token)\n",
            "  Trial 3/3... ✓ 180.16ms (6.01ms/token)\n",
            "\n",
            "--- Testing Output Length: 500 tokens ---\n",
            "  Trial 1/3... ✓ 4791.14ms (9.58ms/token)\n",
            "  Trial 2/3... ✓ 5165.88ms (10.33ms/token)\n",
            "  Trial 3/3... ✓ 150.29ms (5.78ms/token)\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENTS COMPLETE\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}